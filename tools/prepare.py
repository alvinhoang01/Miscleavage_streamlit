from pyteomics import fasta, parser
from tqdm import tqdm
import os
import sqlite3
import pandas as pd
import tempfile
import shutil
import streamlit as st

def get_peptides(param):
    """
    Function to digest a FASTA file into peptides and store the results in an SQLite database.
    Now optimized for cloud environments using a temporary folder.
    """

    fasta_path = param['fasta_path']
    enzyme = param['enzyme']
    missed_cleavages = int(param['missed_cleavage'])
    min_length = int(param['min_length'])
    max_length = int(param['max_length'])
    m_cleavege = bool(param['m_cleavage'])

    # ‚úÖ Fix enzyme rule warning
    if enzyme == "trypsin/p":
        enzyme = r'[KR](?!P)'

    # ‚úÖ Create a temporary directory
    temp_dir = tempfile.mkdtemp()
    sqlite_path = os.path.join(temp_dir, "peptides.sqlite")
    print(f"üìÇ Using temporary directory: {temp_dir}")

    # ‚úÖ Connect to SQLite and create table
    conn = sqlite3.connect(sqlite_path)
    cursor = conn.cursor()
    cursor.execute("DROP TABLE IF EXISTS peptides;")  # Remove existing table
    cursor.execute("CREATE TABLE peptides (peptide TEXT, protein TEXT);")
    conn.commit()

    print("üîç Reading FASTA file and processing proteins...")

    batch_data = []  # ‚úÖ Store small batches of peptides before inserting into SQLite
    batch_size = 5000  # ‚úÖ Control memory usage by writing in chunks

    # ‚úÖ Process FASTA file (main digestion step)
    with fasta.read(fasta_path) as entries:
        for header, sequence in tqdm(entries, desc="Processing Proteins"):
            peptides = parser.xcleave(
                sequence, enzyme,
                missed_cleavages=missed_cleavages,
                min_length=min_length,
                regex=True
            )

            protein = header.split(" ")[0].split("|")[-1]
            for start, peptide in peptides:
                if len(peptide) <= max_length:
                    pre_aa = sequence[start - 1] if start > 0 else "_"
                    post_aa = sequence[start + len(peptide)] if start + len(peptide) < len(sequence) else "_"
                    protein_info = f"{protein}:{start}:{pre_aa}:{post_aa}"

                    batch_data.append((peptide, protein_info))

                # ‚úÖ Insert batch into SQLite to free memory
                if len(batch_data) >= batch_size:
                    cursor.executemany("INSERT INTO peptides VALUES (?, ?);", batch_data)
                    conn.commit()
                    batch_data = []  # Clear batch

    print(f"‚úÖ Total unique peptides stored BEFORE m_cleavege: {len(batch_data)}")

    # ‚úÖ If `m_cleavege` flag is enabled, process again
    if m_cleavege:
        print("üîÑ Running m_cleavage processing...")

        with fasta.read(fasta_path) as entries:
            for header, sequence in tqdm(entries, desc="Processing m_cleavege Proteins"):
                peptides = parser.xcleave(
                    sequence[1:], enzyme,
                    missed_cleavages=missed_cleavages,
                    min_length=min_length,
                    regex=True
                )
                protein = header.split(" ")[0].split("|")[-1]

                for start, peptide in peptides:
                    if len(peptide) <= max_length:
                        pre_aa = sequence[start] if start > 0 else sequence[0]
                        post_aa = sequence[start + 1 + len(peptide)] if start + 1 + len(peptide) < len(sequence) else "_"
                        protein_info = f"{protein}:{start+1}:{pre_aa}:{post_aa}"

                        batch_data.append((peptide, protein_info))

                    # ‚úÖ Insert batch into SQLite to free memory
                    if len(batch_data) >= batch_size:
                        cursor.executemany("INSERT INTO peptides VALUES (?, ?);", batch_data)
                        conn.commit()
                        batch_data = []  # Clear batch

    print(f"‚úÖ Total unique peptides stored AFTER m_cleavege: {len(batch_data)}")

    # ‚úÖ Final batch insert (in case there are leftovers)
    if batch_data:
        cursor.executemany("INSERT INTO peptides VALUES (?, ?);", batch_data)
        conn.commit()

    conn.close()

    # ‚úÖ Print Summary
    print(f"‚úÖ Peptides written to `{sqlite_path}`.")
    print(f"üìÅ SQLite file size: {os.path.getsize(sqlite_path) / 1024:.2f} KB")

    return sqlite_path, temp_dir  # ‚úÖ Return both SQLite file and temp directory
